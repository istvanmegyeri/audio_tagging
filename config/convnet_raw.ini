#Experiments
[core.experiments.TrainModel]
db_reader = db_reader.RawData
compile_params = comp_params
fit_params = fit_params
batch_size = 32
generator_params = generator_params
m_path = saved_models/ConvNet.h5
model = models.ConvNet

[generator_params]
workers = 1
use_multiprocessing = False
max_queue_size = 10

[test_dg_params]


[db_reader.RawData]
labels = res/labels.npz
features = res/raw_features_16_10s.npz
test_size = 0.1
batch_size = 32
train_generator_params = traing_params
test_generator_params = testg_params
#label_smoothing = 0.1

[traing_params]
speedchange_sigma = 0
pitchchange_sigma = 0
noise_sigma = 0.001

[testg_params]
speedchange_sigma = 0
pitchchange_sigma = 0
noise_sigma = 0

##################Model settings##############################

[models.ConvNet]
nb_classes = 80
regularizer = l2
regularizer.l = 1e-6
#dropout = 0,0,0,0.5,0.5
dropout = 1.0,1.0,1.0,0.5,0.5
n_filters = 32

[fit_params]
epochs = 200
verbose = 1
callbacks = core.object_holders.CallbackHolder

[core.object_holders.CallbackHolder]
object_name = keras.callbacks.EarlyStopping,core.metrics.MultiClassMetrics

[core.metrics.MultiClassMetrics]

[keras.callbacks.EarlyStopping]
monitor = val_loss
min_delta = 1e-4
patience = 10
verbose = 1
#restore_best_weights=True

[keras.callbacks.TensorBoard]
log_dir = saved_models/logs/

[comp_params]
optimizer = keras.optimizers.Adam
loss = keras.losses.binary_crossentropy

